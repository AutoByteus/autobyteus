### Automated Coding Workflow Database

#### 1. Story/Feature Description:

1.1. **Background**: 
An AI-driven automated coding workflow, powered by Large Language Models like ChatGPT or LLama2, has been designed to emulate the traditional software engineering process. At each workflow step, the AI agent sends a predefined prompt to the LLM. The LLM responds, and the agent parses this response. The parsed results, which form text documents such as feature code design docs, subtasks, and requirement documents, need systematic storage and association with their respective workflow context in a database.

1.2. **User Persona**: 
AI Coding Agent (Interacting with models like ChatGPT or LLama2):
  - Sends predefined prompts to the LLM at each workflow step.
  - Parses the LLM's responses to generate and interpret requirements, produce feature code design documents, and create and manage subtasks.

1.3. **User Journey**: 
For each workflow step, the AI coding agent sends a predefined prompt to the LLM. Upon receiving the response, the agent parses it to produce the desired output, such as a requirement or design document. These parsed documents, and their linkage with specific workflow steps, are then stored in a PostgreSQL database for future retrieval and reference.

#### 2. Requirements:

2.1. **Functional Requirements**:

- **Database Operations**:
   - Store parsed text documents in the PostgreSQL database.
   - Retrieve parsed text documents based on workflow context.
   - Update existing parsed text documents.
   - Delete parsed text documents.

- **Workflow Association**:
   - Associate a parsed document with its relevant workflow step (e.g., requirement_step, design_step).
   - Fetch all parsed documents linked to a specific workflow step.
   
- **Document Structure**:
   - Support for pure text documents, which are parsed results from LLM interactions, without any specific format or nested structures.

- **Interaction with LLM**:
   - Store the predefined prompts sent to the LLM for each workflow step.
   - Store the raw response from the LLM before parsing.


